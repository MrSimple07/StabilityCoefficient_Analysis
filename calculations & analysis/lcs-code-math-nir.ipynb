{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d80337b5",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-05-10T13:00:04.613170Z",
     "iopub.status.busy": "2024-05-10T13:00:04.612695Z",
     "iopub.status.idle": "2024-05-10T13:00:05.513172Z",
     "shell.execute_reply": "2024-05-10T13:00:05.511528Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 0.914502,
     "end_time": "2024-05-10T13:00:05.516158",
     "exception": false,
     "start_time": "2024-05-10T13:00:04.601656",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/gemma-lcs/generated_results_Gemma_Lcs.csv\n",
      "/kaggle/input/nir-generated-answers/generated_results_with_f1_llama2_chegeka.csv\n",
      "/kaggle/input/nir-generated-answers/chgk_generated_results_with_f1_mistral.csv\n",
      "/kaggle/input/nir-generated-answers/chgk_generated_results_Gemma.csv\n",
      "/kaggle/input/nir-generated-answers/chgk_generated_results_tinyLlama.csv\n",
      "/kaggle/input/nir-generated-answers/chgk_generated_results_with_f1_gemma.csv\n",
      "/kaggle/input/nir-generated-answers/generated_results_Gemma_Lcs.csv\n",
      "/kaggle/input/nir-generated-answers/chegeka-2ndsemester-nir (2).ipynb\n",
      "/kaggle/input/nir-generated-answers/lcs_vikhr_generated_results.csv\n",
      "/kaggle/input/nir-generated-answers/generated_results_llama2_chegeka.csv\n",
      "/kaggle/input/nir-generated-answers/chgk_generated_results_mistral.csv\n",
      "/kaggle/input/nir-generated-answers/lcs_llama2_generated_results.csv\n",
      "/kaggle/input/nir-generated-answers/generated_results_Vikhr_7B_chegeka.csv\n",
      "/kaggle/input/nir-generated-answers/lcs_tinyllama_generated_results.csv\n",
      "/kaggle/input/nir-generated-answers/lcs_mistral_generated_results.csv\n",
      "/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1/config.json\n",
      "/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1/pytorch_model-00002-of-00002.bin\n",
      "/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1/tokenizer.json\n",
      "/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1/tokenizer_config.json\n",
      "/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1/pytorch_model.bin.index.json\n",
      "/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1/pytorch_model-00001-of-00002.bin\n",
      "/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1/special_tokens_map.json\n",
      "/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1/.gitattributes\n",
      "/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1/tokenizer.model\n",
      "/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1/generation_config.json\n",
      "/kaggle/input/gemma/transformers/2b/2/model.safetensors.index.json\n",
      "/kaggle/input/gemma/transformers/2b/2/gemma-2b.gguf\n",
      "/kaggle/input/gemma/transformers/2b/2/config.json\n",
      "/kaggle/input/gemma/transformers/2b/2/model-00001-of-00002.safetensors\n",
      "/kaggle/input/gemma/transformers/2b/2/model-00002-of-00002.safetensors\n",
      "/kaggle/input/gemma/transformers/2b/2/tokenizer.json\n",
      "/kaggle/input/gemma/transformers/2b/2/tokenizer_config.json\n",
      "/kaggle/input/gemma/transformers/2b/2/special_tokens_map.json\n",
      "/kaggle/input/gemma/transformers/2b/2/.gitattributes\n",
      "/kaggle/input/gemma/transformers/2b/2/tokenizer.model\n",
      "/kaggle/input/gemma/transformers/2b/2/generation_config.json\n",
      "/kaggle/input/llama-2/pytorch/7b-hf/1/model.safetensors.index.json\n",
      "/kaggle/input/llama-2/pytorch/7b-hf/1/config.json\n",
      "/kaggle/input/llama-2/pytorch/7b-hf/1/model-00001-of-00002.safetensors\n",
      "/kaggle/input/llama-2/pytorch/7b-hf/1/Responsible-Use-Guide.pdf\n",
      "/kaggle/input/llama-2/pytorch/7b-hf/1/model-00002-of-00002.safetensors\n",
      "/kaggle/input/llama-2/pytorch/7b-hf/1/pytorch_model-00002-of-00002.bin\n",
      "/kaggle/input/llama-2/pytorch/7b-hf/1/README.md\n",
      "/kaggle/input/llama-2/pytorch/7b-hf/1/USE_POLICY.md\n",
      "/kaggle/input/llama-2/pytorch/7b-hf/1/tokenizer.json\n",
      "/kaggle/input/llama-2/pytorch/7b-hf/1/tokenizer_config.json\n",
      "/kaggle/input/llama-2/pytorch/7b-hf/1/pytorch_model.bin.index.json\n",
      "/kaggle/input/llama-2/pytorch/7b-hf/1/LICENSE.txt\n",
      "/kaggle/input/llama-2/pytorch/7b-hf/1/pytorch_model-00001-of-00002.bin\n",
      "/kaggle/input/llama-2/pytorch/7b-hf/1/special_tokens_map.json\n",
      "/kaggle/input/llama-2/pytorch/7b-hf/1/.gitattributes\n",
      "/kaggle/input/llama-2/pytorch/7b-hf/1/tokenizer.model\n",
      "/kaggle/input/llama-2/pytorch/7b-hf/1/added_tokens.json\n",
      "/kaggle/input/llama-2/pytorch/7b-hf/1/generation_config.json\n",
      "/kaggle/input/llama-2/pytorch/7b-chat/1/consolidated.00.pth\n",
      "/kaggle/input/llama-2/pytorch/7b-chat/1/tokenizer_checklist.chk\n",
      "/kaggle/input/llama-2/pytorch/7b-chat/1/Responsible-Use-Guide.pdf\n",
      "/kaggle/input/llama-2/pytorch/7b-chat/1/params.json\n",
      "/kaggle/input/llama-2/pytorch/7b-chat/1/README.md\n",
      "/kaggle/input/llama-2/pytorch/7b-chat/1/USE_POLICY.md\n",
      "/kaggle/input/llama-2/pytorch/7b-chat/1/checklist.chk\n",
      "/kaggle/input/llama-2/pytorch/7b-chat/1/LICENSE.txt\n",
      "/kaggle/input/llama-2/pytorch/7b-chat/1/.gitattributes\n",
      "/kaggle/input/llama-2/pytorch/7b-chat/1/tokenizer.model\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4012e10f",
   "metadata": {
    "papermill": {
     "duration": 0.007566,
     "end_time": "2024-05-10T13:00:05.531874",
     "exception": false,
     "start_time": "2024-05-10T13:00:05.524308",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Models - Chegeka\n",
    "\n",
    "1. Gemma\n",
    "2. Mistral\n",
    "3. Llama 2\n",
    "4. Vikhr\n",
    "5. Tiny LLama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "faa4b8a1",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-05-10T13:00:05.549982Z",
     "iopub.status.busy": "2024-05-10T13:00:05.549434Z",
     "iopub.status.idle": "2024-05-10T13:02:20.929545Z",
     "shell.execute_reply": "2024-05-10T13:02:20.928042Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 135.392535,
     "end_time": "2024-05-10T13:02:20.932641",
     "exception": false,
     "start_time": "2024-05-10T13:00:05.540106",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\r\n",
      "  Downloading langchain-0.1.19-py3-none-any.whl.metadata (13 kB)\r\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (6.0.1)\r\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.0.25)\r\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (3.9.1)\r\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (4.0.3)\r\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain) (0.6.4)\r\n",
      "Collecting langchain-community<0.1,>=0.0.38 (from langchain)\r\n",
      "  Downloading langchain_community-0.0.38-py3-none-any.whl.metadata (8.7 kB)\r\n",
      "Collecting langchain-core<0.2.0,>=0.1.52 (from langchain)\r\n",
      "  Downloading langchain_core-0.1.52-py3-none-any.whl.metadata (5.9 kB)\r\n",
      "Collecting langchain-text-splitters<0.1,>=0.0.1 (from langchain)\r\n",
      "  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl.metadata (2.0 kB)\r\n",
      "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\r\n",
      "  Downloading langsmith-0.1.56-py3-none-any.whl.metadata (13 kB)\r\n",
      "Requirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.26.4)\r\n",
      "Requirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.5.3)\r\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.31.0)\r\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (8.2.3)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.3)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\r\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.1)\r\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\r\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.2.0,>=0.1.52->langchain) (1.33)\r\n",
      "Collecting packaging<24.0,>=23.2 (from langchain-core<0.2.0,>=0.1.52->langchain)\r\n",
      "  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\r\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\r\n",
      "  Downloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (49 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (0.6.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (2.14.6)\r\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (4.9.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2024.2.2)\r\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\r\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.52->langchain) (2.4)\r\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\r\n",
      "Downloading langchain-0.1.19-py3-none-any.whl (1.0 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading langchain_community-0.0.38-py3-none-any.whl (2.0 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading langchain_core-0.1.52-py3-none-any.whl (302 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.9/302.9 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\r\n",
      "Downloading langsmith-0.1.56-py3-none-any.whl (120 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.8/120.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading packaging-23.2-py3-none-any.whl (53 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: packaging, orjson, langsmith, langchain-core, langchain-text-splitters, langchain-community, langchain\r\n",
      "  Attempting uninstall: packaging\r\n",
      "    Found existing installation: packaging 21.3\r\n",
      "    Uninstalling packaging-21.3:\r\n",
      "      Successfully uninstalled packaging-21.3\r\n",
      "  Attempting uninstall: orjson\r\n",
      "    Found existing installation: orjson 3.9.10\r\n",
      "    Uninstalling orjson-3.9.10:\r\n",
      "      Successfully uninstalled orjson-3.9.10\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "keras-cv 0.8.2 requires keras-core, which is not installed.\r\n",
      "keras-nlp 0.9.3 requires keras-core, which is not installed.\r\n",
      "tensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\r\n",
      "apache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\r\n",
      "apache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\r\n",
      "apache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 15.0.2 which is incompatible.\r\n",
      "google-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 23.2 which is incompatible.\r\n",
      "jupyterlab 4.1.6 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\r\n",
      "jupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\r\n",
      "libpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "momepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "osmnx 1.9.2 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "spopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.2.1 which is incompatible.\r\n",
      "ydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed langchain-0.1.19 langchain-community-0.0.38 langchain-core-0.1.52 langchain-text-splitters-0.0.1 langsmith-0.1.56 orjson-3.10.3 packaging-23.2\r\n",
      "Collecting peft\r\n",
      "  Downloading peft-0.10.0-py3-none-any.whl.metadata (13 kB)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (23.2)\r\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\r\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.1)\r\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.1.2+cpu)\r\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.39.3)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.1)\r\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.29.3)\r\n",
      "Requirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.3)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.22.2)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.13.1)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.2.0)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.31.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.9.0)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.12)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.2)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2023.12.25)\r\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.15.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.2.2)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\r\n",
      "Downloading peft-0.10.0-py3-none-any.whl (199 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: peft\r\n",
      "Successfully installed peft-0.10.0\r\n",
      "Collecting trl\r\n",
      "  Downloading trl-0.8.6-py3-none-any.whl.metadata (11 kB)\r\n",
      "Requirement already satisfied: torch>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from trl) (2.1.2+cpu)\r\n",
      "Requirement already satisfied: transformers>=4.31.0 in /opt/conda/lib/python3.10/site-packages (from trl) (4.39.3)\r\n",
      "Requirement already satisfied: numpy>=1.18.2 in /opt/conda/lib/python3.10/site-packages (from trl) (1.26.4)\r\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from trl) (0.29.3)\r\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from trl) (2.18.0)\r\n",
      "Collecting tyro>=0.5.11 (from trl)\r\n",
      "  Downloading tyro-0.8.4-py3-none-any.whl.metadata (7.9 kB)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (3.13.1)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (4.9.0)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (1.12)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (3.1.2)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (2024.2.0)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (0.22.2)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (23.2)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (6.0.1)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (2023.12.25)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (2.31.0)\r\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (0.15.2)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (0.4.3)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (4.66.1)\r\n",
      "Requirement already satisfied: docstring-parser>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl) (0.15)\r\n",
      "Requirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl) (13.7.0)\r\n",
      "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl)\r\n",
      "  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\r\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate->trl) (5.9.3)\r\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (15.0.2)\r\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (0.6)\r\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (0.3.8)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (2.2.2)\r\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (3.4.1)\r\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (0.70.16)\r\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (3.9.1)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (23.2.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (6.0.4)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (1.9.3)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (1.4.1)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (1.3.1)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (4.0.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl) (2024.2.2)\r\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.17.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.4.0->trl) (2.1.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl) (2023.3.post1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl) (2023.4)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.4.0->trl) (1.3.0)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->trl) (1.16.0)\r\n",
      "Downloading trl-0.8.6-py3-none-any.whl (245 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.2/245.2 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading tyro-0.8.4-py3-none-any.whl (102 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.4/102.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\r\n",
      "Installing collected packages: shtab, tyro, trl\r\n",
      "Successfully installed shtab-1.7.1 trl-0.8.6 tyro-0.8.4\r\n",
      "Requirement already satisfied: shap in /opt/conda/lib/python3.10/site-packages (0.44.1)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from shap) (1.26.4)\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from shap) (1.11.4)\r\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from shap) (1.2.2)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from shap) (2.2.2)\r\n",
      "Requirement already satisfied: tqdm>=4.27.0 in /opt/conda/lib/python3.10/site-packages (from shap) (4.66.1)\r\n",
      "Requirement already satisfied: packaging>20.9 in /opt/conda/lib/python3.10/site-packages (from shap) (23.2)\r\n",
      "Requirement already satisfied: slicer==0.0.7 in /opt/conda/lib/python3.10/site-packages (from shap) (0.0.7)\r\n",
      "Requirement already satisfied: numba in /opt/conda/lib/python3.10/site-packages (from shap) (0.58.1)\r\n",
      "Requirement already satisfied: cloudpickle in /opt/conda/lib/python3.10/site-packages (from shap) (2.2.1)\r\n",
      "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba->shap) (0.41.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->shap) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->shap) (2023.3.post1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->shap) (2023.4)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->shap) (1.4.0)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->shap) (3.2.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->shap) (1.16.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain\n",
    "!pip install peft\n",
    "!pip install trl\n",
    "\n",
    "!pip install -q -U transformers\n",
    "!pip install -q accelerate\n",
    "!pip install -q -i https://pypi.org/simple/ bitsandbytes\n",
    "!pip install -q -U datasets\n",
    "!pip install shap\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ae041cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T13:02:20.964449Z",
     "iopub.status.busy": "2024-05-10T13:02:20.963242Z",
     "iopub.status.idle": "2024-05-10T13:02:20.969812Z",
     "shell.execute_reply": "2024-05-10T13:02:20.968387Z"
    },
    "papermill": {
     "duration": 0.024999,
     "end_time": "2024-05-10T13:02:20.972422",
     "exception": false,
     "start_time": "2024-05-10T13:02:20.947423",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gm = \"/kaggle/input/gemma/transformers/2b/2\"\n",
    "mistral = '/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1'\n",
    "llama2 = '/kaggle/input/llama-2/pytorch/7b-hf/1'\n",
    "vikhr = 'Vikhrmodels/Vikhr-7B-instruct_merged'\n",
    "tinyllama = 'TinyLlama/TinyLlama-1.1B-Chat-v1.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fe315b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T13:02:21.001433Z",
     "iopub.status.busy": "2024-05-10T13:02:20.999931Z",
     "iopub.status.idle": "2024-05-10T13:02:43.158390Z",
     "shell.execute_reply": "2024-05-10T13:02:43.157157Z"
    },
    "papermill": {
     "duration": 22.176063,
     "end_time": "2024-05-10T13:02:43.161533",
     "exception": false,
     "start_time": "2024-05-10T13:02:20.985470",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-10 13:02:30.776525: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-10 13:02:30.776670: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-10 13:02:30.954135: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import transformers\n",
    "from transformers import (AutoModelForCausalLM, \n",
    "                          AutoTokenizer, \n",
    "                          BitsAndBytesConfig, \n",
    "                          TrainingArguments, \n",
    "                          pipeline, \n",
    "                          logging)\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, PeftConfig\n",
    "import bitsandbytes as bnb\n",
    "from trl import SFTTrainer\n",
    "\n",
    "from sklearn.metrics import (accuracy_score, \n",
    "                             classification_report, \n",
    "                             confusion_matrix)\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27687494",
   "metadata": {
    "papermill": {
     "duration": 0.01309,
     "end_time": "2024-05-10T13:02:43.188615",
     "exception": false,
     "start_time": "2024-05-10T13:02:43.175525",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Gemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1096e4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T13:02:43.217811Z",
     "iopub.status.busy": "2024-05-10T13:02:43.216919Z",
     "iopub.status.idle": "2024-05-10T13:03:20.834344Z",
     "shell.execute_reply": "2024-05-10T13:03:20.832856Z"
    },
    "papermill": {
     "duration": 37.635506,
     "end_time": "2024-05-10T13:03:20.837521",
     "exception": false,
     "start_time": "2024-05-10T13:02:43.202015",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gemma's activation function should be approximate GeLU and not exact GeLU.\n",
      "Changing the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5918a2869ce14e739fd9d092879a7f28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(\"text-generation\", model=gm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115c7fee",
   "metadata": {
    "papermill": {
     "duration": 0.013235,
     "end_time": "2024-05-10T13:03:20.865452",
     "exception": false,
     "start_time": "2024-05-10T13:03:20.852217",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d444ebbb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T13:03:20.894872Z",
     "iopub.status.busy": "2024-05-10T13:03:20.894438Z",
     "iopub.status.idle": "2024-05-10T13:03:22.492454Z",
     "shell.execute_reply": "2024-05-10T13:03:22.488977Z"
    },
    "papermill": {
     "duration": 1.621134,
     "end_time": "2024-05-10T13:03:22.500187",
     "exception": true,
     "start_time": "2024-05-10T13:03:20.879053",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Using `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` and the latest version of bitsandbytes: `pip install -i https://pypi.org/simple/ bitsandbytes`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 12\u001b[0m\n\u001b[1;32m      3\u001b[0m compute_dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(torch, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat16\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m bnb_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(\n\u001b[1;32m      6\u001b[0m     load_in_4bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      7\u001b[0m     bnb_4bit_use_double_quant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m      8\u001b[0m     bnb_4bit_quant_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnf4\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m     bnb_4bit_compute_dtype\u001b[38;5;241m=\u001b[39mcompute_dtype,\n\u001b[1;32m     10\u001b[0m )\n\u001b[0;32m---> 12\u001b[0m model_mistral \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmistral\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m model_mistral\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     19\u001b[0m model_mistral\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:563\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    562\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:3165\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3162\u001b[0m     hf_quantizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 3165\u001b[0m     \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\n\u001b[1;32m   3167\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3168\u001b[0m     torch_dtype \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_torch_dtype(torch_dtype)\n\u001b[1;32m   3169\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_device_map(device_map)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:62\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.validate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidate_environment\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_accelerate_available() \u001b[38;5;129;01mand\u001b[39;00m is_bitsandbytes_available()):\n\u001b[0;32m---> 62\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     63\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     64\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand the latest version of bitsandbytes: `pip install -i https://pypi.org/simple/ bitsandbytes`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     65\u001b[0m         )\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_tf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_flax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     68\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     69\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConverting into 4-bit or 8-bit weights from tf/flax weights is currently not supported, please make\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     70\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m sure the weights are in PyTorch format.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     71\u001b[0m         )\n",
      "\u001b[0;31mImportError\u001b[0m: Using `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` and the latest version of bitsandbytes: `pip install -i https://pypi.org/simple/ bitsandbytes`"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    ")\n",
    "\n",
    "model_mistral = AutoModelForCausalLM.from_pretrained(\n",
    "    mistral,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config, \n",
    ")\n",
    "\n",
    "model_mistral.config.use_cache = False\n",
    "model_mistral.config.pretraining_tp = 1\n",
    "\n",
    "max_seq_length = 2048\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(mistral)\n",
    "# EOS_TOKEN = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ca76fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T10:21:13.910680Z",
     "iopub.status.busy": "2024-05-10T10:21:13.910272Z",
     "iopub.status.idle": "2024-05-10T10:21:13.915912Z",
     "shell.execute_reply": "2024-05-10T10:21:13.914754Z",
     "shell.execute_reply.started": "2024-05-10T10:21:13.910642Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipe = pipeline(\"text-generation\", \n",
    "                model=model_mistral, \n",
    "                tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac71faa",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Llama 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4c6bc8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T11:04:41.575305Z",
     "iopub.status.busy": "2024-05-10T11:04:41.574613Z",
     "iopub.status.idle": "2024-05-10T11:07:37.786299Z",
     "shell.execute_reply": "2024-05-10T11:07:37.785486Z",
     "shell.execute_reply.started": "2024-05-10T11:04:41.575273Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "llama2 = '/kaggle/input/llama-2/pytorch/7b-hf/1'\n",
    "# llama2 = 'meta-llama/Llama-2-7b'\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    ")\n",
    "\n",
    "model_llama2 = AutoModelForCausalLM.from_pretrained(\n",
    "    llama2,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config, \n",
    ")\n",
    "\n",
    "model_llama2.config.use_cache = False\n",
    "model_llama2.config.pretraining_tp = 1\n",
    "\n",
    "max_seq_length = 2048\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(llama2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3674b07b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T11:07:47.728246Z",
     "iopub.status.busy": "2024-05-10T11:07:47.727845Z",
     "iopub.status.idle": "2024-05-10T11:07:47.733808Z",
     "shell.execute_reply": "2024-05-10T11:07:47.732757Z",
     "shell.execute_reply.started": "2024-05-10T11:07:47.728214Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipe = pipeline(\"text-generation\", \n",
    "                model=model_llama2, \n",
    "                tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6502161e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Vikhr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6cefeb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T11:54:06.451429Z",
     "iopub.status.busy": "2024-05-10T11:54:06.450717Z",
     "iopub.status.idle": "2024-05-10T11:56:23.406397Z",
     "shell.execute_reply": "2024-05-10T11:56:23.405347Z",
     "shell.execute_reply.started": "2024-05-10T11:54:06.451396Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "from transformers import pipeline\n",
    "\n",
    "vikhr = 'Vikhrmodels/Vikhr-7B-instruct_0.4'\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    ")\n",
    "\n",
    "model_vikhr = AutoModelForCausalLM.from_pretrained(\n",
    "    vikhr,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config, \n",
    ")\n",
    "\n",
    "model_vikhr.config.use_cache = False\n",
    "model_vikhr.config.pretraining_tp = 1\n",
    "\n",
    "max_seq_length = 2048\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(vikhr)\n",
    "# EOS_TOKEN = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e24a5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T11:56:34.370474Z",
     "iopub.status.busy": "2024-05-10T11:56:34.370098Z",
     "iopub.status.idle": "2024-05-10T11:56:34.383783Z",
     "shell.execute_reply": "2024-05-10T11:56:34.382400Z",
     "shell.execute_reply.started": "2024-05-10T11:56:34.370446Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", \n",
    "                model=model_vikhr, \n",
    "                tokenizer= tokenizer, \n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06979a88",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# TinyLlama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992046ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T12:46:21.299337Z",
     "iopub.status.busy": "2024-05-10T12:46:21.298521Z",
     "iopub.status.idle": "2024-05-10T12:46:32.993203Z",
     "shell.execute_reply": "2024-05-10T12:46:32.992451Z",
     "shell.execute_reply.started": "2024-05-10T12:46:21.299305Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(\"text-generation\", model=tinyllama)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb3d777",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Data sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fc14c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T23:32:37.501897Z",
     "iopub.status.busy": "2024-05-09T23:32:37.501609Z",
     "iopub.status.idle": "2024-05-09T23:32:42.081539Z",
     "shell.execute_reply": "2024-05-09T23:32:42.080688Z",
     "shell.execute_reply.started": "2024-05-09T23:32:37.501873Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"ai-forever/MERA\", 'lcs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cf4180",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-05-09T23:32:42.083025Z",
     "iopub.status.busy": "2024-05-09T23:32:42.082758Z",
     "iopub.status.idle": "2024-05-09T23:32:42.098380Z",
     "shell.execute_reply": "2024-05-09T23:32:42.097310Z",
     "shell.execute_reply.started": "2024-05-09T23:32:42.083000Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "total_prompts = dataset[\"public_test\"].num_rows\n",
    "random_indices = random.sample(range(total_prompts), 10)\n",
    "\n",
    "random_prompts = []\n",
    "inputs = []\n",
    "outputs = []\n",
    "\n",
    "for idx in random_indices:\n",
    "  prompt_text = dataset[\"public_test\"][idx]\n",
    "  inputs_text = dataset['public_test'][idx]['inputs']\n",
    "  outputs_text = dataset['public_test'][idx]['outputs']  \n",
    "\n",
    "  random_prompts.append(prompt_text)\n",
    "  inputs.append(inputs_text)\n",
    "  outputs.append(outputs_text)\n",
    "\n",
    "\n",
    "# Print the 10 random prompts\n",
    "print(\"10 Random Prompts from MERA 'lcs' dataset:\")\n",
    "for prompt in random_prompts:\n",
    "  print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de4a934",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T11:07:55.052576Z",
     "iopub.status.busy": "2024-05-10T11:07:55.052182Z",
     "iopub.status.idle": "2024-05-10T11:07:55.059755Z",
     "shell.execute_reply": "2024-05-10T11:07:55.058541Z",
     "shell.execute_reply.started": "2024-05-10T11:07:55.052546Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lcs_task_collection = [\n",
    "    {'input': \"CZUWAQSJOS LTHWJNYRZ\"},\n",
    "    {'input': \"LONUPTDGMYUOTLSKOEPSVRUT SVTZVMLSGZWOWSMPHZYYFVRERFYXUI\"},\n",
    "    {'input': \"HZCEEHBLMDTZFLBQZJV NPRNUVHPXBGGACZJHZXQPFLU\"},\n",
    "    {'input': \"VRJOZGMJQTSFQCRZAEUKJRWJICVLQMD MXVOYOFUURVUYXJMCLLVFXXT\"},\n",
    "    {'input': \"TZIWMBYEVSDHKOVAIXTZ DUAPNZHVFSVIPGLEMNFOJ\"},\n",
    "    {'input': \"FFOBNUN ZPSXDKMEYDVCRXMMYPWCMKVTQKH\"},\n",
    "    {'input': \"LBKLB DNXMDOHDA\"},\n",
    "    {'input': \"PZWLQBSQWQNELMGOVWRNOXS WIWUSEVJKKPQXRWVEDJVTMHQODQJ\"},\n",
    "    {'input': \"NIKVSXCMRUQWJNMFVLOEKDOYFZZUSDJ JPJUPOALIYCCHYA\"},\n",
    "    {'input': \"ZJYYBHRZD QZLVSBBRFYUWUZADYYAPTEEIGKG\"}\n",
    "]\n",
    "\n",
    "\n",
    "lcs_collection_answers = [\n",
    "    {'answer': \"2\"},\n",
    "    {'answer': \"9\"},\n",
    "    {'answer': \"6\"},\n",
    "    {'answer': \"8\"},\n",
    "    {'answer': \"5\"},\n",
    "    {'answer': \"0\"},\n",
    "    {'answer': \"0\"},\n",
    "    {'answer': \"7\"},\n",
    "    {'answer': \"3\"},\n",
    "    {'answer': \"5\"}\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278f60a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T11:07:57.232510Z",
     "iopub.status.busy": "2024-05-10T11:07:57.232105Z",
     "iopub.status.idle": "2024-05-10T11:07:57.749677Z",
     "shell.execute_reply": "2024-05-10T11:07:57.748918Z",
     "shell.execute_reply.started": "2024-05-10T11:07:57.232482Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "lcs_prompt_collection = [\n",
    "    PromptTemplate(\n",
    "        template='''Даны две строки в качестве входных данных:\\n\"{inputs}\"\\nУкажите длину их наибольшей общей подпоследовательности. Между буквами могут быть пропуски. Запишите только цифру, отвечающую на вопрос, никаких дополнительных рассуждений приводить не нужно. Ответ: ''',\n",
    "        input_variables= ['input']\n",
    "    ),\n",
    "    PromptTemplate(\n",
    "        template='''По заданным строкам:\\n\"{inputs}\"\\nОпределите длину наибольшей общей подпоследовательности. Учтите возможность пропуска символов. Запишите только цифру, отвечающую на вопрос, никаких дополнительных рассуждений приводить не нужно. Ответ: ''',\n",
    "        input_variables=['input']\n",
    "    ),\n",
    "    PromptTemplate(\n",
    "        template='''Даны две строки: \"{inputs}\". Укажите длину наибольшей общей подпоследовательности. Пропуск символов допустим. Запишите только цифру, отвечающую на вопрос, никаких дополнительных рассуждений приводить не нужно. Ответ: ''',\n",
    "        input_variables=['input']\n",
    "    ),\n",
    "    PromptTemplate(\n",
    "        template='''По двум введенным строкам:\\n\"{inputs}\"\\nУкажите длину наибольшей общей подпоследовательности. Разрешены пропуски символов. Запишите только цифру, отвечающую на вопрос, никаких дополнительных рассуждений приводить не нужно. Ответ: ''',\n",
    "        input_variables=['input']\n",
    "    ),\n",
    "    PromptTemplate(\n",
    "        template='''Предоставлены строки:\\n\"{inputs}\"\\nНайдите длину наибольшей общей подпоследовательности. Можно использовать пропуск символов. Запишите только цифру, отвечающую на вопрос, никаких дополнительных рассуждений приводить не нужно. Ответ: ''',\n",
    "        input_variables=['input']\n",
    "    ),\n",
    "    PromptTemplate(\n",
    "        template='''Заданы две строки:\\n\"{inputs}\"\\nОпределите, сколько символов составляет наибольшая общая подпоследовательность. Пропуск символов допустим. Запишите только цифру, отвечающую на вопрос, никаких дополнительных рассуждений приводить не нужно. Ответ: ''',\n",
    "        input_variables=['input']\n",
    "    ),\n",
    "    PromptTemplate(\n",
    "        template='''По данным строкам:\\n\"{inputs}\"\\nНайдите длину наибольшей общей подпоследовательности. Разрешено использование пропусков. Запишите только цифру, отвечающую на вопрос, никаких дополнительных рассуждений приводить не нужно. Ответ: ''',\n",
    "        input_variables=['input']\n",
    "    ),\n",
    "    PromptTemplate(\n",
    "        template='''У вас есть две строки:\\n\"{inputs}\"\\nНеобходимо определить длину наибольшей общей подпоследовательности. Пропуски символов разрешены. Запишите только цифру, отвечающую на вопрос, никаких дополнительных рассуждений приводить не нужно. Ответ: ''',\n",
    "        input_variables=['input']\n",
    "    ),\n",
    "    PromptTemplate(\n",
    "        template='''По строкам:\\n\"{inputs}\"\\nУкажите, сколько символов составляет наибольшая общая подпоследовательность. Можно использовать пропуск символов. Запишите только цифру, отвечающую на вопрос, никаких дополнительных рассуждений приводить не нужно. Ответ: ''',\n",
    "        input_variables=['input']\n",
    "    ),\n",
    "    PromptTemplate(\n",
    "        template='''Две строки предоставлены:\\n\"{inputs}\"\\nНайдите длину наибольшей общей подпоследовательности. Разрешается пропуск символов. Запишите только цифру, отвечающую на вопрос, никаких дополнительных рассуждений приводить не нужно. Ответ: ''',\n",
    "        input_variables=['input']\n",
    "    ),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfeceda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T12:46:45.903319Z",
     "iopub.status.busy": "2024-05-10T12:46:45.902666Z",
     "iopub.status.idle": "2024-05-10T12:46:45.911679Z",
     "shell.execute_reply": "2024-05-10T12:46:45.910511Z",
     "shell.execute_reply.started": "2024-05-10T12:46:45.903285Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_texts(prompt_collection, task_collection, answer_collection, pipe):\n",
    "    results = []\n",
    "\n",
    "    for task, answer in zip(task_collection, answer_collection):\n",
    "        for prompt_template in prompt_collection:\n",
    "            prompt_text = prompt_template.template.format(inputs=task['input'])\n",
    "            generated_text = pipe(prompt_text, return_full_text=False, max_new_tokens=100)[0]['generated_text']\n",
    "            print(\"Generated Text:\", generated_text)  \n",
    "            results.append({'question': task['input'], 'prompt': prompt_text, 'expected_answer': answer['answer'], 'generated_text': generated_text})\n",
    "\n",
    "    df_results = pd.DataFrame(results)\n",
    "    df_results.to_csv('generated_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e2fef5",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-05-10T00:58:35.620021Z",
     "iopub.status.busy": "2024-05-10T00:58:35.619237Z",
     "iopub.status.idle": "2024-05-10T03:01:42.001154Z",
     "shell.execute_reply": "2024-05-10T03:01:42.000170Z",
     "shell.execute_reply.started": "2024-05-10T00:58:35.619985Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Gemma generated Answers\n",
    "\n",
    "process_texts(lcs_prompt_collection, lcs_task_collection, lcs_collection_answers, pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b6baa2",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-05-10T10:23:32.628722Z",
     "iopub.status.busy": "2024-05-10T10:23:32.627742Z",
     "iopub.status.idle": "2024-05-10T10:24:59.609377Z",
     "shell.execute_reply": "2024-05-10T10:24:59.608150Z",
     "shell.execute_reply.started": "2024-05-10T10:23:32.628686Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mistral generated Answers\n",
    "\n",
    "process_texts(lcs_prompt_collection, lcs_task_collection, lcs_collection_answers, pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a501afd",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-05-10T11:08:15.281309Z",
     "iopub.status.busy": "2024-05-10T11:08:15.280303Z",
     "iopub.status.idle": "2024-05-10T11:51:45.962339Z",
     "shell.execute_reply": "2024-05-10T11:51:45.961343Z",
     "shell.execute_reply.started": "2024-05-10T11:08:15.281256Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Llama2 generated Answers\n",
    "\n",
    "process_texts(lcs_prompt_collection, lcs_task_collection, lcs_collection_answers, pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb24323f",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-05-10T11:56:50.272725Z",
     "iopub.status.busy": "2024-05-10T11:56:50.272001Z",
     "iopub.status.idle": "2024-05-10T12:41:32.189092Z",
     "shell.execute_reply": "2024-05-10T12:41:32.188121Z",
     "shell.execute_reply.started": "2024-05-10T11:56:50.272690Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Vikhr generated Answers\n",
    "\n",
    "process_texts(lcs_prompt_collection, lcs_task_collection, lcs_collection_answers, pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d6fba9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T12:47:00.355823Z",
     "iopub.status.busy": "2024-05-10T12:47:00.355415Z",
     "iopub.status.idle": "2024-05-10T12:51:21.567205Z",
     "shell.execute_reply": "2024-05-10T12:51:21.566119Z",
     "shell.execute_reply.started": "2024-05-10T12:47:00.355787Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TinyLLama generated Answers\n",
    "\n",
    "process_texts(lcs_prompt_collection, lcs_task_collection, lcs_collection_answers, pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd01ef4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T12:52:41.370343Z",
     "iopub.status.busy": "2024-05-10T12:52:41.369583Z",
     "iopub.status.idle": "2024-05-10T12:52:41.387144Z",
     "shell.execute_reply": "2024-05-10T12:52:41.386143Z",
     "shell.execute_reply.started": "2024-05-10T12:52:41.370312Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_results = pd.read_csv('/kaggle/working/generated_results.csv')\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1740ca1b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T12:44:09.079045Z",
     "iopub.status.busy": "2024-05-10T12:44:09.078656Z",
     "iopub.status.idle": "2024-05-10T12:44:09.085199Z",
     "shell.execute_reply": "2024-05-10T12:44:09.084148Z",
     "shell.execute_reply.started": "2024-05-10T12:44:09.079014Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdc3a91",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990101c7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Calculating Accuracy"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4979079,
     "sourceId": 8374478,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4975445,
     "sourceId": 8374582,
     "sourceType": "datasetVersion"
    },
    {
     "modelInstanceId": 3090,
     "sourceId": 4295,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 3900,
     "sourceId": 5112,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 6216,
     "sourceId": 11384,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelInstanceId": 3092,
     "sourceId": 4297,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 203.885865,
   "end_time": "2024-05-10T13:03:25.248231",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-10T13:00:01.362366",
   "version": "2.5.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "2774128c0324489cb413a3f55eac4584": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_a731ed4194a241ca9f43e003de1d4106",
       "placeholder": "​",
       "style": "IPY_MODEL_9636c8d899034c75b9d9b302bcb40e9b",
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "5918a2869ce14e739fd9d092879a7f28": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_2774128c0324489cb413a3f55eac4584",
        "IPY_MODEL_cb1ae67cb6774d98a984b0aab6c7dcc2",
        "IPY_MODEL_961e4fa0695245e486d54d8beee1ddaf"
       ],
       "layout": "IPY_MODEL_5b9339e2aefb49efa533303e993901df"
      }
     },
     "5b9339e2aefb49efa533303e993901df": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "788fb9ad2eb242a6b57136af2a5f5fba": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "89918b310e4f4c6c9a2f8e67be2e47b4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "961e4fa0695245e486d54d8beee1ddaf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_788fb9ad2eb242a6b57136af2a5f5fba",
       "placeholder": "​",
       "style": "IPY_MODEL_9e63d2aa2df046aabac4d16f626af6ea",
       "value": " 2/2 [00:35&lt;00:00, 14.76s/it]"
      }
     },
     "9636c8d899034c75b9d9b302bcb40e9b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "9e63d2aa2df046aabac4d16f626af6ea": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "a731ed4194a241ca9f43e003de1d4106": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "bbd12b8f96ce480fa07dfd693f191e28": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "cb1ae67cb6774d98a984b0aab6c7dcc2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_89918b310e4f4c6c9a2f8e67be2e47b4",
       "max": 2.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_bbd12b8f96ce480fa07dfd693f191e28",
       "value": 2.0
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
