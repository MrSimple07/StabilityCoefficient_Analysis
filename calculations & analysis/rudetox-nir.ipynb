{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1889658",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-05-10T13:18:30.373241Z",
     "iopub.status.busy": "2024-05-10T13:18:30.372833Z",
     "iopub.status.idle": "2024-05-10T13:18:31.537648Z",
     "shell.execute_reply": "2024-05-10T13:18:31.536543Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 1.183351,
     "end_time": "2024-05-10T13:18:31.541232",
     "exception": false,
     "start_time": "2024-05-10T13:18:30.357881",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/nir-generated-answers/generated_results_with_f1_llama2_chegeka.csv\n",
      "/kaggle/input/nir-generated-answers/chgk_generated_results_with_f1_mistral.csv\n",
      "/kaggle/input/nir-generated-answers/chgk_generated_results_Gemma.csv\n",
      "/kaggle/input/nir-generated-answers/chgk_generated_results_tinyLlama.csv\n",
      "/kaggle/input/nir-generated-answers/chgk_generated_results_with_f1_gemma.csv\n",
      "/kaggle/input/nir-generated-answers/generated_results_Gemma_Lcs.csv\n",
      "/kaggle/input/nir-generated-answers/chegeka-2ndsemester-nir (2).ipynb\n",
      "/kaggle/input/nir-generated-answers/lcs_vikhr_generated_results.csv\n",
      "/kaggle/input/nir-generated-answers/generated_results_llama2_chegeka.csv\n",
      "/kaggle/input/nir-generated-answers/chgk_generated_results_mistral.csv\n",
      "/kaggle/input/nir-generated-answers/lcs_llama2_generated_results.csv\n",
      "/kaggle/input/nir-generated-answers/generated_results_Vikhr_7B_chegeka.csv\n",
      "/kaggle/input/nir-generated-answers/lcs_tinyllama_generated_results.csv\n",
      "/kaggle/input/nir-generated-answers/lcs_mistral_generated_results.csv\n",
      "/kaggle/input/llama-2/pytorch/7b-hf/1/model.safetensors.index.json\n",
      "/kaggle/input/llama-2/pytorch/7b-hf/1/config.json\n",
      "/kaggle/input/llama-2/pytorch/7b-hf/1/model-00001-of-00002.safetensors\n",
      "/kaggle/input/llama-2/pytorch/7b-hf/1/Responsible-Use-Guide.pdf\n",
      "/kaggle/input/llama-2/pytorch/7b-hf/1/model-00002-of-00002.safetensors\n",
      "/kaggle/input/llama-2/pytorch/7b-hf/1/pytorch_model-00002-of-00002.bin\n",
      "/kaggle/input/llama-2/pytorch/7b-hf/1/README.md\n",
      "/kaggle/input/llama-2/pytorch/7b-hf/1/USE_POLICY.md\n",
      "/kaggle/input/llama-2/pytorch/7b-hf/1/tokenizer.json\n",
      "/kaggle/input/llama-2/pytorch/7b-hf/1/tokenizer_config.json\n",
      "/kaggle/input/llama-2/pytorch/7b-hf/1/pytorch_model.bin.index.json\n",
      "/kaggle/input/llama-2/pytorch/7b-hf/1/LICENSE.txt\n",
      "/kaggle/input/llama-2/pytorch/7b-hf/1/pytorch_model-00001-of-00002.bin\n",
      "/kaggle/input/llama-2/pytorch/7b-hf/1/special_tokens_map.json\n",
      "/kaggle/input/llama-2/pytorch/7b-hf/1/.gitattributes\n",
      "/kaggle/input/llama-2/pytorch/7b-hf/1/tokenizer.model\n",
      "/kaggle/input/llama-2/pytorch/7b-hf/1/added_tokens.json\n",
      "/kaggle/input/llama-2/pytorch/7b-hf/1/generation_config.json\n",
      "/kaggle/input/llama-2/pytorch/7b-chat/1/consolidated.00.pth\n",
      "/kaggle/input/llama-2/pytorch/7b-chat/1/tokenizer_checklist.chk\n",
      "/kaggle/input/llama-2/pytorch/7b-chat/1/Responsible-Use-Guide.pdf\n",
      "/kaggle/input/llama-2/pytorch/7b-chat/1/params.json\n",
      "/kaggle/input/llama-2/pytorch/7b-chat/1/README.md\n",
      "/kaggle/input/llama-2/pytorch/7b-chat/1/USE_POLICY.md\n",
      "/kaggle/input/llama-2/pytorch/7b-chat/1/checklist.chk\n",
      "/kaggle/input/llama-2/pytorch/7b-chat/1/LICENSE.txt\n",
      "/kaggle/input/llama-2/pytorch/7b-chat/1/.gitattributes\n",
      "/kaggle/input/llama-2/pytorch/7b-chat/1/tokenizer.model\n",
      "/kaggle/input/gemma/transformers/2b/2/model.safetensors.index.json\n",
      "/kaggle/input/gemma/transformers/2b/2/gemma-2b.gguf\n",
      "/kaggle/input/gemma/transformers/2b/2/config.json\n",
      "/kaggle/input/gemma/transformers/2b/2/model-00001-of-00002.safetensors\n",
      "/kaggle/input/gemma/transformers/2b/2/model-00002-of-00002.safetensors\n",
      "/kaggle/input/gemma/transformers/2b/2/tokenizer.json\n",
      "/kaggle/input/gemma/transformers/2b/2/tokenizer_config.json\n",
      "/kaggle/input/gemma/transformers/2b/2/special_tokens_map.json\n",
      "/kaggle/input/gemma/transformers/2b/2/.gitattributes\n",
      "/kaggle/input/gemma/transformers/2b/2/tokenizer.model\n",
      "/kaggle/input/gemma/transformers/2b/2/generation_config.json\n",
      "/kaggle/input/gemma-lcs/generated_results_Gemma_Lcs.csv\n",
      "/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1/config.json\n",
      "/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1/pytorch_model-00002-of-00002.bin\n",
      "/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1/tokenizer.json\n",
      "/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1/tokenizer_config.json\n",
      "/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1/pytorch_model.bin.index.json\n",
      "/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1/pytorch_model-00001-of-00002.bin\n",
      "/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1/special_tokens_map.json\n",
      "/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1/.gitattributes\n",
      "/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1/tokenizer.model\n",
      "/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1/generation_config.json\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d761dd8",
   "metadata": {
    "papermill": {
     "duration": 0.012141,
     "end_time": "2024-05-10T13:18:31.565604",
     "exception": false,
     "start_time": "2024-05-10T13:18:31.553463",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Models - Chegeka\n",
    "\n",
    "1. Gemma\n",
    "2. Mistral\n",
    "3. Llama 2\n",
    "4. Vikhr\n",
    "5. Tiny LLama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d020daea",
   "metadata": {
    "papermill": {
     "duration": 0.012622,
     "end_time": "2024-05-10T13:18:31.590682",
     "exception": false,
     "start_time": "2024-05-10T13:18:31.578060",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Task - ruDetox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea8d2e11",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-05-10T13:18:31.618836Z",
     "iopub.status.busy": "2024-05-10T13:18:31.618060Z",
     "iopub.status.idle": "2024-05-10T13:21:14.731297Z",
     "shell.execute_reply": "2024-05-10T13:21:14.729753Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 163.130984,
     "end_time": "2024-05-10T13:21:14.734661",
     "exception": false,
     "start_time": "2024-05-10T13:18:31.603677",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\r\n",
      "  Downloading langchain-0.1.19-py3-none-any.whl.metadata (13 kB)\r\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (6.0.1)\r\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.0.25)\r\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (3.9.1)\r\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (4.0.3)\r\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain) (0.6.4)\r\n",
      "Collecting langchain-community<0.1,>=0.0.38 (from langchain)\r\n",
      "  Downloading langchain_community-0.0.38-py3-none-any.whl.metadata (8.7 kB)\r\n",
      "Collecting langchain-core<0.2.0,>=0.1.52 (from langchain)\r\n",
      "  Downloading langchain_core-0.1.52-py3-none-any.whl.metadata (5.9 kB)\r\n",
      "Collecting langchain-text-splitters<0.1,>=0.0.1 (from langchain)\r\n",
      "  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl.metadata (2.0 kB)\r\n",
      "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\r\n",
      "  Downloading langsmith-0.1.56-py3-none-any.whl.metadata (13 kB)\r\n",
      "Requirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.26.4)\r\n",
      "Requirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.5.3)\r\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.31.0)\r\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (8.2.3)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.3)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\r\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.1)\r\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\r\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.2.0,>=0.1.52->langchain) (1.33)\r\n",
      "Collecting packaging<24.0,>=23.2 (from langchain-core<0.2.0,>=0.1.52->langchain)\r\n",
      "  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\r\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\r\n",
      "  Downloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (49 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (0.6.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (2.14.6)\r\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (4.9.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2024.2.2)\r\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\r\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.52->langchain) (2.4)\r\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\r\n",
      "Downloading langchain-0.1.19-py3-none-any.whl (1.0 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading langchain_community-0.0.38-py3-none-any.whl (2.0 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading langchain_core-0.1.52-py3-none-any.whl (302 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.9/302.9 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\r\n",
      "Downloading langsmith-0.1.56-py3-none-any.whl (120 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.8/120.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading packaging-23.2-py3-none-any.whl (53 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: packaging, orjson, langsmith, langchain-core, langchain-text-splitters, langchain-community, langchain\r\n",
      "  Attempting uninstall: packaging\r\n",
      "    Found existing installation: packaging 21.3\r\n",
      "    Uninstalling packaging-21.3:\r\n",
      "      Successfully uninstalled packaging-21.3\r\n",
      "  Attempting uninstall: orjson\r\n",
      "    Found existing installation: orjson 3.9.10\r\n",
      "    Uninstalling orjson-3.9.10:\r\n",
      "      Successfully uninstalled orjson-3.9.10\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "keras-cv 0.8.2 requires keras-core, which is not installed.\r\n",
      "keras-nlp 0.9.3 requires keras-core, which is not installed.\r\n",
      "tensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\r\n",
      "apache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\r\n",
      "apache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\r\n",
      "apache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 15.0.2 which is incompatible.\r\n",
      "google-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 23.2 which is incompatible.\r\n",
      "jupyterlab 4.1.6 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\r\n",
      "jupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\r\n",
      "libpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "momepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "osmnx 1.9.2 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "spopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.2.1 which is incompatible.\r\n",
      "ydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed langchain-0.1.19 langchain-community-0.0.38 langchain-core-0.1.52 langchain-text-splitters-0.0.1 langsmith-0.1.56 orjson-3.10.3 packaging-23.2\r\n",
      "Collecting peft\r\n",
      "  Downloading peft-0.10.0-py3-none-any.whl.metadata (13 kB)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (23.2)\r\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\r\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.1)\r\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.1.2+cpu)\r\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.39.3)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.1)\r\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.29.3)\r\n",
      "Requirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.3)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.22.2)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.13.1)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.2.0)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.31.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.9.0)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.12)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.2)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2023.12.25)\r\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.15.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.2.2)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\r\n",
      "Downloading peft-0.10.0-py3-none-any.whl (199 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: peft\r\n",
      "Successfully installed peft-0.10.0\r\n",
      "Collecting trl\r\n",
      "  Downloading trl-0.8.6-py3-none-any.whl.metadata (11 kB)\r\n",
      "Requirement already satisfied: torch>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from trl) (2.1.2+cpu)\r\n",
      "Requirement already satisfied: transformers>=4.31.0 in /opt/conda/lib/python3.10/site-packages (from trl) (4.39.3)\r\n",
      "Requirement already satisfied: numpy>=1.18.2 in /opt/conda/lib/python3.10/site-packages (from trl) (1.26.4)\r\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from trl) (0.29.3)\r\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from trl) (2.18.0)\r\n",
      "Collecting tyro>=0.5.11 (from trl)\r\n",
      "  Downloading tyro-0.8.4-py3-none-any.whl.metadata (7.9 kB)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (3.13.1)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (4.9.0)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (1.12)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (3.1.2)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (2024.2.0)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (0.22.2)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (23.2)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (6.0.1)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (2023.12.25)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (2.31.0)\r\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (0.15.2)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (0.4.3)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (4.66.1)\r\n",
      "Requirement already satisfied: docstring-parser>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl) (0.15)\r\n",
      "Requirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl) (13.7.0)\r\n",
      "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl)\r\n",
      "  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\r\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate->trl) (5.9.3)\r\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (15.0.2)\r\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (0.6)\r\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (0.3.8)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (2.2.2)\r\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (3.4.1)\r\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (0.70.16)\r\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (3.9.1)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (23.2.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (6.0.4)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (1.9.3)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (1.4.1)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (1.3.1)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (4.0.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl) (2024.2.2)\r\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.17.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.4.0->trl) (2.1.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl) (2023.3.post1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl) (2023.4)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.4.0->trl) (1.3.0)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->trl) (1.16.0)\r\n",
      "Downloading trl-0.8.6-py3-none-any.whl (245 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.2/245.2 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading tyro-0.8.4-py3-none-any.whl (102 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.4/102.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\r\n",
      "Installing collected packages: shtab, tyro, trl\r\n",
      "Successfully installed shtab-1.7.1 trl-0.8.6 tyro-0.8.4\r\n",
      "Requirement already satisfied: shap in /opt/conda/lib/python3.10/site-packages (0.44.1)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from shap) (1.26.4)\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from shap) (1.11.4)\r\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from shap) (1.2.2)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from shap) (2.2.2)\r\n",
      "Requirement already satisfied: tqdm>=4.27.0 in /opt/conda/lib/python3.10/site-packages (from shap) (4.66.1)\r\n",
      "Requirement already satisfied: packaging>20.9 in /opt/conda/lib/python3.10/site-packages (from shap) (23.2)\r\n",
      "Requirement already satisfied: slicer==0.0.7 in /opt/conda/lib/python3.10/site-packages (from shap) (0.0.7)\r\n",
      "Requirement already satisfied: numba in /opt/conda/lib/python3.10/site-packages (from shap) (0.58.1)\r\n",
      "Requirement already satisfied: cloudpickle in /opt/conda/lib/python3.10/site-packages (from shap) (2.2.1)\r\n",
      "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba->shap) (0.41.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->shap) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->shap) (2023.3.post1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->shap) (2023.4)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->shap) (1.4.0)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->shap) (3.2.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->shap) (1.16.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain\n",
    "!pip install peft\n",
    "!pip install trl\n",
    "\n",
    "!pip install -q -U transformers\n",
    "!pip install -q accelerate\n",
    "!pip install -q -i https://pypi.org/simple/ bitsandbytes\n",
    "!pip install -q -U datasets\n",
    "!pip install shap\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "226be323",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T13:21:14.778252Z",
     "iopub.status.busy": "2024-05-10T13:21:14.777223Z",
     "iopub.status.idle": "2024-05-10T13:21:14.785318Z",
     "shell.execute_reply": "2024-05-10T13:21:14.783852Z"
    },
    "papermill": {
     "duration": 0.033682,
     "end_time": "2024-05-10T13:21:14.788215",
     "exception": false,
     "start_time": "2024-05-10T13:21:14.754533",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gm = \"/kaggle/input/gemma/transformers/2b/2\"\n",
    "mistral = '/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1'\n",
    "llama2 = '/kaggle/input/llama-2/pytorch/7b-hf/1'\n",
    "vikhr = 'Vikhrmodels/Vikhr-7B-instruct_merged'\n",
    "tinyllama = 'TinyLlama/TinyLlama-1.1B-Chat-v1.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d36f4d40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T13:21:14.828255Z",
     "iopub.status.busy": "2024-05-10T13:21:14.827781Z",
     "iopub.status.idle": "2024-05-10T13:21:40.037677Z",
     "shell.execute_reply": "2024-05-10T13:21:40.036116Z"
    },
    "papermill": {
     "duration": 25.233972,
     "end_time": "2024-05-10T13:21:40.040897",
     "exception": false,
     "start_time": "2024-05-10T13:21:14.806925",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-10 13:21:26.060550: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-10 13:21:26.060723: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-10 13:21:26.253443: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import transformers\n",
    "from transformers import (AutoModelForCausalLM, \n",
    "                          AutoTokenizer, \n",
    "                          BitsAndBytesConfig, \n",
    "                          TrainingArguments, \n",
    "                          pipeline, \n",
    "                          logging)\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, PeftConfig\n",
    "import bitsandbytes as bnb\n",
    "from trl import SFTTrainer\n",
    "\n",
    "from sklearn.metrics import (accuracy_score, \n",
    "                             classification_report, \n",
    "                             confusion_matrix)\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494730f7",
   "metadata": {
    "papermill": {
     "duration": 0.018524,
     "end_time": "2024-05-10T13:21:40.078795",
     "exception": false,
     "start_time": "2024-05-10T13:21:40.060271",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Gemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cf0ef23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T13:21:40.119691Z",
     "iopub.status.busy": "2024-05-10T13:21:40.118727Z",
     "iopub.status.idle": "2024-05-10T13:22:35.840271Z",
     "shell.execute_reply": "2024-05-10T13:22:35.838715Z"
    },
    "papermill": {
     "duration": 55.74513,
     "end_time": "2024-05-10T13:22:35.843457",
     "exception": false,
     "start_time": "2024-05-10T13:21:40.098327",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gemma's activation function should be approximate GeLU and not exact GeLU.\n",
      "Changing the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e941a0a0c454b7eb5d58f4c877e25d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(\"text-generation\", model=gm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf9f2a5",
   "metadata": {
    "papermill": {
     "duration": 0.019999,
     "end_time": "2024-05-10T13:22:35.884401",
     "exception": false,
     "start_time": "2024-05-10T13:22:35.864402",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f7d73a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T13:22:35.927986Z",
     "iopub.status.busy": "2024-05-10T13:22:35.927578Z",
     "iopub.status.idle": "2024-05-10T13:22:38.061975Z",
     "shell.execute_reply": "2024-05-10T13:22:38.055342Z"
    },
    "papermill": {
     "duration": 2.168145,
     "end_time": "2024-05-10T13:22:38.073417",
     "exception": true,
     "start_time": "2024-05-10T13:22:35.905272",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Using `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` and the latest version of bitsandbytes: `pip install -i https://pypi.org/simple/ bitsandbytes`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 12\u001b[0m\n\u001b[1;32m      3\u001b[0m compute_dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(torch, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat16\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m bnb_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(\n\u001b[1;32m      6\u001b[0m     load_in_4bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      7\u001b[0m     bnb_4bit_use_double_quant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m      8\u001b[0m     bnb_4bit_quant_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnf4\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m     bnb_4bit_compute_dtype\u001b[38;5;241m=\u001b[39mcompute_dtype,\n\u001b[1;32m     10\u001b[0m )\n\u001b[0;32m---> 12\u001b[0m model_mistral \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmistral\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m model_mistral\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     19\u001b[0m model_mistral\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:563\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    562\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:3165\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3162\u001b[0m     hf_quantizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 3165\u001b[0m     \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\n\u001b[1;32m   3167\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3168\u001b[0m     torch_dtype \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_torch_dtype(torch_dtype)\n\u001b[1;32m   3169\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_device_map(device_map)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:62\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.validate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidate_environment\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_accelerate_available() \u001b[38;5;129;01mand\u001b[39;00m is_bitsandbytes_available()):\n\u001b[0;32m---> 62\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     63\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     64\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand the latest version of bitsandbytes: `pip install -i https://pypi.org/simple/ bitsandbytes`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     65\u001b[0m         )\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_tf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_flax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     68\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     69\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConverting into 4-bit or 8-bit weights from tf/flax weights is currently not supported, please make\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     70\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m sure the weights are in PyTorch format.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     71\u001b[0m         )\n",
      "\u001b[0;31mImportError\u001b[0m: Using `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` and the latest version of bitsandbytes: `pip install -i https://pypi.org/simple/ bitsandbytes`"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    ")\n",
    "\n",
    "model_mistral = AutoModelForCausalLM.from_pretrained(\n",
    "    mistral,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config, \n",
    ")\n",
    "\n",
    "model_mistral.config.use_cache = False\n",
    "model_mistral.config.pretraining_tp = 1\n",
    "\n",
    "max_seq_length = 2048\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(mistral)\n",
    "# EOS_TOKEN = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247bd9d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T10:21:13.910680Z",
     "iopub.status.busy": "2024-05-10T10:21:13.910272Z",
     "iopub.status.idle": "2024-05-10T10:21:13.915912Z",
     "shell.execute_reply": "2024-05-10T10:21:13.914754Z",
     "shell.execute_reply.started": "2024-05-10T10:21:13.910642Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipe = pipeline(\"text-generation\", \n",
    "                model=model_mistral, \n",
    "                tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f119ee",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Llama 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca00b30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T11:04:41.575305Z",
     "iopub.status.busy": "2024-05-10T11:04:41.574613Z",
     "iopub.status.idle": "2024-05-10T11:07:37.786299Z",
     "shell.execute_reply": "2024-05-10T11:07:37.785486Z",
     "shell.execute_reply.started": "2024-05-10T11:04:41.575273Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "llama2 = '/kaggle/input/llama-2/pytorch/7b-hf/1'\n",
    "# llama2 = 'meta-llama/Llama-2-7b'\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    ")\n",
    "\n",
    "model_llama2 = AutoModelForCausalLM.from_pretrained(\n",
    "    llama2,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config, \n",
    ")\n",
    "\n",
    "model_llama2.config.use_cache = False\n",
    "model_llama2.config.pretraining_tp = 1\n",
    "\n",
    "max_seq_length = 2048\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(llama2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d78b40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T11:07:47.728246Z",
     "iopub.status.busy": "2024-05-10T11:07:47.727845Z",
     "iopub.status.idle": "2024-05-10T11:07:47.733808Z",
     "shell.execute_reply": "2024-05-10T11:07:47.732757Z",
     "shell.execute_reply.started": "2024-05-10T11:07:47.728214Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipe = pipeline(\"text-generation\", \n",
    "                model=model_llama2, \n",
    "                tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d888d30b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Vikhr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf777bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T11:54:06.451429Z",
     "iopub.status.busy": "2024-05-10T11:54:06.450717Z",
     "iopub.status.idle": "2024-05-10T11:56:23.406397Z",
     "shell.execute_reply": "2024-05-10T11:56:23.405347Z",
     "shell.execute_reply.started": "2024-05-10T11:54:06.451396Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "from transformers import pipeline\n",
    "\n",
    "vikhr = 'Vikhrmodels/Vikhr-7B-instruct_0.4'\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    ")\n",
    "\n",
    "model_vikhr = AutoModelForCausalLM.from_pretrained(\n",
    "    vikhr,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config, \n",
    ")\n",
    "\n",
    "model_vikhr.config.use_cache = False\n",
    "model_vikhr.config.pretraining_tp = 1\n",
    "\n",
    "max_seq_length = 2048\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(vikhr)\n",
    "# EOS_TOKEN = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3aee656",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T11:56:34.370474Z",
     "iopub.status.busy": "2024-05-10T11:56:34.370098Z",
     "iopub.status.idle": "2024-05-10T11:56:34.383783Z",
     "shell.execute_reply": "2024-05-10T11:56:34.382400Z",
     "shell.execute_reply.started": "2024-05-10T11:56:34.370446Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", \n",
    "                model=model_vikhr, \n",
    "                tokenizer= tokenizer, \n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4688d2e3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# TinyLlama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dd2298",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T12:46:21.299337Z",
     "iopub.status.busy": "2024-05-10T12:46:21.298521Z",
     "iopub.status.idle": "2024-05-10T12:46:32.993203Z",
     "shell.execute_reply": "2024-05-10T12:46:32.992451Z",
     "shell.execute_reply.started": "2024-05-10T12:46:21.299305Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(\"text-generation\", model=tinyllama)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acc1c9a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Data sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb59f148",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T13:00:58.021052Z",
     "iopub.status.busy": "2024-05-10T13:00:58.020395Z",
     "iopub.status.idle": "2024-05-10T13:01:05.317302Z",
     "shell.execute_reply": "2024-05-10T13:01:05.316279Z",
     "shell.execute_reply.started": "2024-05-10T13:00:58.021021Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"ai-forever/MERA\", 'rudetox')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980fc6e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T13:03:50.069324Z",
     "iopub.status.busy": "2024-05-10T13:03:50.068899Z",
     "iopub.status.idle": "2024-05-10T13:03:50.078196Z",
     "shell.execute_reply": "2024-05-10T13:03:50.076908Z",
     "shell.execute_reply.started": "2024-05-10T13:03:50.069291Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2336e61d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T13:05:39.995497Z",
     "iopub.status.busy": "2024-05-10T13:05:39.995032Z",
     "iopub.status.idle": "2024-05-10T13:05:40.007965Z",
     "shell.execute_reply": "2024-05-10T13:05:40.006827Z",
     "shell.execute_reply.started": "2024-05-10T13:05:39.995464Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "total_prompts = dataset[\"train\"].num_rows\n",
    "random_indices = random.sample(range(total_prompts), 10)\n",
    "\n",
    "random_prompts = []\n",
    "inputs = []\n",
    "outputs = []\n",
    "\n",
    "for idx in random_indices:\n",
    "  prompt_text = dataset[\"train\"][idx]\n",
    "  inputs_text = dataset['train'][idx]['inputs']\n",
    "  outputs_text = dataset['train'][idx]['outputs']  \n",
    "\n",
    "  random_prompts.append(prompt_text)\n",
    "  inputs.append(inputs_text)\n",
    "  outputs.append(outputs_text)\n",
    "\n",
    "\n",
    "# Print the 10 random prompts\n",
    "print(\"10 Random Prompts from MERA 'ruDetox' dataset:\")\n",
    "for prompt in random_prompts:\n",
    "  print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179bfb1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T13:10:32.354975Z",
     "iopub.status.busy": "2024-05-10T13:10:32.354558Z",
     "iopub.status.idle": "2024-05-10T13:10:32.362299Z",
     "shell.execute_reply": "2024-05-10T13:10:32.361357Z",
     "shell.execute_reply.started": "2024-05-10T13:10:32.354943Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rudetox_task_collection = [\n",
    "    {'toxic_comment': 'Ну а зачем мне овуляшка, которая свалит в декрет, какой-нибудь неадекватный верун-сектант или бывший зек?'},\n",
    "    {'toxic_comment': 'пидора гнойного на нары уебок'},\n",
    "    {'toxic_comment': 'а все остальные в масках !!!!боятся твари!!!!! понимают что здохнет скоро мосейчук!!!!!'},\n",
    "    {'toxic_comment': 'я вообще ничего не писала, а вот ты постоянно пишешь, что там сука, давай, доказывал'},\n",
    "    {'toxic_comment': 'юебище лесное терпеть не могу эту породу срамота тьфу'},\n",
    "    {'toxic_comment': '- Туяра,покрась мне волосы.- Не могу,у меня нестабильное эмоциональное состояние :(- Ц,заебала'},\n",
    "    {'toxic_comment': 'вот он секрет российских политиков, еблом торговать.'},\n",
    "    {'toxic_comment': 'Что это за ёбань? Моллюски?'},\n",
    "    {'toxic_comment': 'затрахали путиноиды гребанные!! все хвалят и хвалят этого президента'},\n",
    "    {'toxic_comment': 'зато его урода лправдывают, а людей невиных гнобят.купленые и судьи и адвокаты.твари.'}\n",
    "]\n",
    "\n",
    "rudetox_collection_answers = [\n",
    "    {'answer': 'Ну а зачем мне девушка, которая уйдет в декрет, сектант или бывший заключенный?'},\n",
    "    {'answer': 'В тюрьму его!'},\n",
    "    {'answer': 'а все остальные в масках !боятся ! понимают что уйдёт скоро мосейчук!!!!!'},\n",
    "    {'answer': 'я вообще ничего не писала, а вот ты постоянно пишешь, что там , давай, доказывал'},\n",
    "    {'answer': 'терпеть не могу эту породу'},\n",
    "    {'answer': 'Туяра, покрась мне волосы. Не могу, у меня нестабильное эмоциональное состояние'},\n",
    "    {'answer': 'Вот он секрет российских политиков,торговать лицом'},\n",
    "    {'answer': 'Это что , моллюски ?'},\n",
    "    {'answer': 'Надоели эти люди!!всё хвалят и хвалят этого президента'},\n",
    "    {'answer': 'Зато его оправдывают ,а людей невинных обижают ,купленные судьи и адвокаты'}\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f55e9c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T13:09:16.765106Z",
     "iopub.status.busy": "2024-05-10T13:09:16.764707Z",
     "iopub.status.idle": "2024-05-10T13:09:17.700434Z",
     "shell.execute_reply": "2024-05-10T13:09:17.699429Z",
     "shell.execute_reply.started": "2024-05-10T13:09:16.765074Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "rudetox_prompt_collection = [\n",
    "    PromptTemplate(input_variables=[\"toxic_comment\"],\n",
    "                  template='''Есть токсичный ответ: \"{toxic_comment}\"\n",
    "Перефразируйте токсичный ответ так, чтобы он стал нетоксичным, сохраняя при этом исходный смысл, орфографию и пунктуацию. Ответ: '''), #1\n",
    "    PromptTemplate(input_variables=[\"toxic_comment\"],\n",
    "                  template='''Токсичный ответ: \"{toxic_comment}\"\n",
    "Преобразуй этот ответ в нетоксичный вариант, который был бы уважительным к собеседнику и не содержал бы никаких оскорблений или угроз, но при этом сохранял бы исходное сообщение, орфографию и пунктуацию. Ответ: '''), #2\n",
    "    PromptTemplate(input_variables=[\"toxic_comment\"],\n",
    "                  template='''Токсичный комментарий: \"{toxic_comment}\"\n",
    "Измени стиль этого комментария так, чтобы он стал нетоксичным, но при этом не потерял своего основного содержания, а также сохранял орфографию и пунктуацию. Ответ: '''), #3\n",
    "    PromptTemplate(input_variables=[\"toxic_comment\"],\n",
    "                  template='''Токсичное сообщение: \"{toxic_comment}\"\n",
    "Преобразуй это сообщение в дружелюбное и уважительное, сохраняя исходное намерение, информацию, орфографию и пунктуацию. Ответ: '''), #4\n",
    "    PromptTemplate(input_variables=[\"toxic_comment\"],\n",
    "                  template='''Токсичный ответ: \"{toxic_comment}\"\n",
    "Преобразуй этот ответ в нетоксичный вариант, который был бы уважительным к собеседнику и не содержал бы никаких оскорблений или угроз, но при этом сохранял бы исходное сообщение, орфографию и пунктуацию. Ответ: '''), #5\n",
    "    PromptTemplate(input_variables=[\"toxic_comment\"],\n",
    "                  template='''Токсичный ответ: \"{toxic_comment}\"\n",
    "Преобразуй этот ответ в нетоксичный вариант, который был бы уважительным к собеседнику и не содержал бы никаких оскорблений или угроз, но при этом сохранял бы исходное сообщение, орфографию и пунктуацию. Ответ: '''), #6\n",
    "    PromptTemplate(input_variables=[\"toxic_comment\"],\n",
    "                  template='''Токсичное замечание: \"{toxic_comment}\"\n",
    "Перепиши это замечание так, чтобы оно было корректным и не содержало никаких оскорблений или уничижительных высказываний, но при этом сохраняло бы свою основную идею, орфографию и пунктуацию. Ответ: '''), #7\n",
    "    PromptTemplate(input_variables=[\"toxic_comment\"],\n",
    "                  template='''Есть токсичный ответ: \"{toxic_comment}\"\n",
    "Перефразируйте токсичный ответ так, чтобы он стал нетоксичным, сохраняя при этом исходный смысл, орфографию и пунктуацию. Ответ: '''), #8\n",
    "    PromptTemplate(input_variables=[\"toxic_comment\"],\n",
    "                  template='''Дано исходное высказывание: \"{toxic_comment}\"\n",
    ", оно является токсичным\n",
    "Преобразуй токсичное содержание в нетоксичное, не теряя первоначального смысла, орфографии и пунктуации. Ответ: '''), #9\n",
    "    PromptTemplate(input_variables=[\"toxic_comment\"],\n",
    "                  template='''Токсичный ответ: \"{toxic_comment}\"\n",
    "Преобразуй этот ответ в нетоксичный вариант, который был бы уважительным к собеседнику и не содержал бы никаких оскорблений или угроз, но при этом сохранял бы исходное сообщение, орфографию и пунктуацию. Ответ: ''') #10\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9db9751",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T12:46:45.903319Z",
     "iopub.status.busy": "2024-05-10T12:46:45.902666Z",
     "iopub.status.idle": "2024-05-10T12:46:45.911679Z",
     "shell.execute_reply": "2024-05-10T12:46:45.910511Z",
     "shell.execute_reply.started": "2024-05-10T12:46:45.903285Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_texts(prompt_collection, task_collection, answer_collection, pipe):\n",
    "    results = []\n",
    "\n",
    "    for task, answer in zip(task_collection, answer_collection):\n",
    "        for prompt_template in prompt_collection:\n",
    "            prompt_text = prompt_template.template.format(toxic_comment=task['toxic_comment'])\n",
    "            generated_text = pipe(prompt_text, return_full_text=False, max_new_tokens=100)[0]['generated_text']\n",
    "            results.append({'question': task['toxic_comment'], 'prompt': prompt_text, 'expected_answer': answer['answer'], 'generated_text': generated_text})\n",
    "\n",
    "    df_results = pd.DataFrame(results)\n",
    "    df_results.to_csv('generated_results.csv', index=False)\n",
    "\n",
    "\n",
    "process_texts(rudetox_prompt_collection, rudetox_task_collection, rudetox_collection_answers, pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4499c61f",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-05-10T00:58:35.620021Z",
     "iopub.status.busy": "2024-05-10T00:58:35.619237Z",
     "iopub.status.idle": "2024-05-10T03:01:42.001154Z",
     "shell.execute_reply": "2024-05-10T03:01:42.000170Z",
     "shell.execute_reply.started": "2024-05-10T00:58:35.619985Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Gemma generated Answers\n",
    "\n",
    "process_texts(lcs_prompt_collection, lcs_task_collection, lcs_collection_answers, pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58ca3db",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-05-10T10:23:32.628722Z",
     "iopub.status.busy": "2024-05-10T10:23:32.627742Z",
     "iopub.status.idle": "2024-05-10T10:24:59.609377Z",
     "shell.execute_reply": "2024-05-10T10:24:59.608150Z",
     "shell.execute_reply.started": "2024-05-10T10:23:32.628686Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mistral generated Answers\n",
    "\n",
    "process_texts(lcs_prompt_collection, lcs_task_collection, lcs_collection_answers, pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7e917e",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-05-10T11:08:15.281309Z",
     "iopub.status.busy": "2024-05-10T11:08:15.280303Z",
     "iopub.status.idle": "2024-05-10T11:51:45.962339Z",
     "shell.execute_reply": "2024-05-10T11:51:45.961343Z",
     "shell.execute_reply.started": "2024-05-10T11:08:15.281256Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Llama2 generated Answers\n",
    "\n",
    "process_texts(lcs_prompt_collection, lcs_task_collection, lcs_collection_answers, pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c31b7d",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-05-10T11:56:50.272725Z",
     "iopub.status.busy": "2024-05-10T11:56:50.272001Z",
     "iopub.status.idle": "2024-05-10T12:41:32.189092Z",
     "shell.execute_reply": "2024-05-10T12:41:32.188121Z",
     "shell.execute_reply.started": "2024-05-10T11:56:50.272690Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Vikhr generated Answers\n",
    "\n",
    "process_texts(lcs_prompt_collection, lcs_task_collection, lcs_collection_answers, pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b48c7cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T12:47:00.355823Z",
     "iopub.status.busy": "2024-05-10T12:47:00.355415Z",
     "iopub.status.idle": "2024-05-10T12:51:21.567205Z",
     "shell.execute_reply": "2024-05-10T12:51:21.566119Z",
     "shell.execute_reply.started": "2024-05-10T12:47:00.355787Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TinyLLama generated Answers\n",
    "\n",
    "process_texts(lcs_prompt_collection, lcs_task_collection, lcs_collection_answers, pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba283e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T12:52:41.370343Z",
     "iopub.status.busy": "2024-05-10T12:52:41.369583Z",
     "iopub.status.idle": "2024-05-10T12:52:41.387144Z",
     "shell.execute_reply": "2024-05-10T12:52:41.386143Z",
     "shell.execute_reply.started": "2024-05-10T12:52:41.370312Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_results = pd.read_csv('/kaggle/working/generated_results.csv')\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d4458e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T12:44:09.079045Z",
     "iopub.status.busy": "2024-05-10T12:44:09.078656Z",
     "iopub.status.idle": "2024-05-10T12:44:09.085199Z",
     "shell.execute_reply": "2024-05-10T12:44:09.084148Z",
     "shell.execute_reply.started": "2024-05-10T12:44:09.079014Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d199af",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b39f3c9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Calculating Accuracy"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 8508383,
     "datasetId": 4979079,
     "sourceId": 8374478,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 8509669,
     "datasetId": 4975445,
     "sourceId": 8375714,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 7771913,
     "modelInstanceId": 6216,
     "sourceId": 11384,
     "sourceType": "modelInstanceVersion"
    },
    {
     "databundleVersionId": 6243007,
     "modelInstanceId": 3090,
     "sourceId": 4295,
     "sourceType": "modelInstanceVersion"
    },
    {
     "databundleVersionId": 6243107,
     "modelInstanceId": 3092,
     "sourceId": 4297,
     "sourceType": "modelInstanceVersion"
    },
    {
     "databundleVersionId": 6745013,
     "modelInstanceId": 3900,
     "sourceId": 5112,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 253.908323,
   "end_time": "2024-05-10T13:22:40.844878",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-10T13:18:26.936555",
   "version": "2.5.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "1220fb9fe1c74cd6b281217263d271e0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "277efa13432a4902af1d3d840fd4c3e1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "67de7570023b4c6db5ed61f8a1add8e5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7e941a0a0c454b7eb5d58f4c877e25d0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_910a3fa8cb7c4ef5a13686bff5cf02f1",
        "IPY_MODEL_ee930dd60c374274a144c27abc3352c5",
        "IPY_MODEL_cef56833ee9d440abe1fe1e86ba13a86"
       ],
       "layout": "IPY_MODEL_8df4f804dcb5407b831af9f19579bfe0"
      }
     },
     "8df4f804dcb5407b831af9f19579bfe0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "910a3fa8cb7c4ef5a13686bff5cf02f1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_9b0cacfb38e74e0495f204b8870a9c55",
       "placeholder": "​",
       "style": "IPY_MODEL_fcdd0a4613c9425a90dbf71abc7463c7",
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "9b0cacfb38e74e0495f204b8870a9c55": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "cef56833ee9d440abe1fe1e86ba13a86": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_67de7570023b4c6db5ed61f8a1add8e5",
       "placeholder": "​",
       "style": "IPY_MODEL_e06fe88d706448708eec0051621f5c02",
       "value": " 2/2 [00:52&lt;00:00, 21.87s/it]"
      }
     },
     "e06fe88d706448708eec0051621f5c02": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "ee930dd60c374274a144c27abc3352c5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_1220fb9fe1c74cd6b281217263d271e0",
       "max": 2.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_277efa13432a4902af1d3d840fd4c3e1",
       "value": 2.0
      }
     },
     "fcdd0a4613c9425a90dbf71abc7463c7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
